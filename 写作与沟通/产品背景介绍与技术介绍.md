# 背景介绍

对于专业视频工作者，剪辑视频的大型软件对工作设备的性能要求极高，所以他们常常选择不便移动办公的高性能台式机，离开了这些高性能设备就无法完成工作。除了此类工作除了对性能要求极高以外，对储存空间要求也非常高。在 5G 快速普及的时代，`4K 60FPS RAW` 规格的视频已经成为视频制作者能接受的最低要求，因为这样的分辨率下才有最大的后期空间。而一个 `1 TB` 的 `SSD` 仅能储存不到 30 分钟的高规格视频。对一些刚起步的小型公司来说，满足这两个方面的需求会是很大的一笔开销。为解决这些问题，本公司推出了包括 Premiere/Da Vinci Resolve/Blender 等大型软件的云服务。

云软件概念非常简单，就是将软件布署到服务器上运行，将云服务器渲染出来的高质量音视频画面，通过流的形式传送到终端，终端上不再需要安装各种大型软件，各种终端例如电视、手机、PC、平板都可以运行。这样我们就不需要关心软件如何适配不同的软硬件平台、终端性能不足等等问题。与此概念类似的是云游戏，在2009年的时候，云游戏就已经出现，美国有家叫 `OnLive` 的公司第一个推出云游戏服务，但是最终在商业上还是失败了，技术最后被索尼公司收购，并运用在 `PS Now` 上。云游戏的概念虽然很好，但技术挑战性非常高，有非常多的技术问题需要解决，`OnLive` 时代可能还比较早，软硬件都还不成熟，所以最后没有能够成功的商业化。到了现在这个时间点，云软件技术开始慢慢成熟，已经具备了商业化的基础。

# 技术介绍

对云端运行软件来说，用户主要会关心延迟问题，如果进行视频剪辑或 `3D` 建模，如果存在几百毫秒的延迟，软件使用体验就会非常差。所以我们最核心的关注点就是要把延迟降低到最小、并且把画质保持在一个相对可以接受的程度。目前我们产品的整体延迟（从用户按下操作按钮到看到画面变化）可以控制到 `50` 毫秒以下，在这样的延迟水平在大多数使用场景下几乎感知不到，画面可以支持到 `720 P/1080 P`，而网络带宽只要 `4 mbps` 以上就可以了。我们的单台服务器可以支持 `20-50` 路的软件并发数量，也就是单台服务器可以同时为 50 个用户提供服务，单个软件并发的整体服务器硬件成本在 `500` 元左右，可以说是非常有竞争力的成本。而 `OnLive` 失败的主要原因是因为硬件成本极高，一台服务器仅能服务一个用户，单个并发用户的成本可能就要上万，在这样的成本水平上要实现商业上的成功是非常困难的。目前我们的项目已经在小范围的内测，主要是 `to B` 的业务，为宽带运营商提供增值软件云服务。

我们的产品主要解决了以下几个技术难点：

## 实时性

软件的整体延迟包括了软件逻辑运算时间、音画渲染的时间，加上编码的延时、网路传输的延时、客户端解码的延时、客户端向服务端发送控制信息的延时，云软件的实时性要达到一个可令用户接受的程度，这个技术挑战极高，也要依靠硬件和网络本身的性能，如果没有足够的带宽也很难做到。

## 虚拟化技术

虚拟化在服务端已经非常成熟，我们有虚拟机技术以及各种容器技术，但是在桌面上就不是那么成熟，普通的虚拟桌面不支持 GPU 的虚拟化，而我们面向的客户非常依赖 GPU 渲染，若没有 GPU 的虚拟化就无法实现云软件，所以虚拟化是需要突破的技术瓶颈。

## 经济性

每个并发用户的服务器硬件成本关系到这个模式能否成功商业化，如果成本超出了用户可接受的范围，那就没有办法实现盈利。

## 运维管理

云软件的运维管理与传统的服务器运维管理不同，因为用到的服务器硬件不同，同时硬件负载极高，这对运维管理提出了新的挑战，所以在技术上就要解决这些问题。

## 平台选择

软件的运行平台多种多样，但是比较适合的只有 Windows 平台。Linux 平台虽然开放，但是几乎没有专业软件的支持，

Windows 软件的虚拟化技术主要是两条路线。一个是虚拟机方案，但主要问题是 GPU 虚拟化技术不成熟，可能需要一些专业级的显卡支持，成本非常高、性能损耗较大，每一个软件都运行一个 Guest OS 非常浪费内存，所以这条方案被我们否决了。同时 Windows 上也缺少可用的容器级技术，我们只能采取 API Hook 方式手工实现虚拟化，我们称之为 Thyme 方案。

Thyme 方案就是把软件所用到的系统 API 全部 hook 接管，让软件认为自己运行在一个正常的 OS 上，但实际上是由我们接管的 OS。这样做的好处是性能损耗小，没有额外的损耗，但是技术难点在于要针对每个 API 做适配，需要对每个软件进行适配，而专业软件通常不开源，软件开发商通常也不会配合第三方去修改代码，需要一些 hack 技术来针对每个软件做适配。

## 技术实现细节

### 图像和声音的采集

图形 API 有 `DirectX 9,10,11,12` 还有 `OpenGL` ，接管这些 API 后就可以把画面重定向到视频编码器，不需要在屏幕上输出。而音频比较简单，只需要接管 `Windows Audio Session API` 。

### 输入操作的虚拟化

技术难度最大的是专业调色键盘，因为这些调色键盘支持的 API 接口比较多样化，比如 `DirectInput/XInput/RawInput`，还有些设备直接读 USB，实现这些 API 的接管工作是比较琐碎的。

### 存储的虚拟划分

+   资源部分，比如执行程序、图片、声音等等。这些资源文件都是只读的，需要一个共享存储来放这些文件，因为这些文件体积比较大，通常一个软件需要 10G 以上的容量，如果全部都放在本地节点上的话，对节点的存储容量要求很大，而且以后更新维护起来也比较困难。所以我们用 NAS 来共享这些文件，这么做的网络 I/O 开销会非常大，后面会介绍如何来优化这一块。

+   用户配置和存档数据等等可变数据，这些数据需要集中化存储，同时可能存在跨机房的访问需求。用户离机房越近延迟越小，所以需要多地、异地部署服务器，让用户在群过漫游访问我们的服务，这需要有跨机房文件共享的能力。

### 其他需要适配的内容

有些专业软件无法后台窗口运行，我们需要通过 `API Hook` 的方式，屏蔽软件的检测。最理想的适配方式是通过 `SDK`，让 `PC` 来适配我们的软件云服务，但目前来说还不实际，因为软件云服务的商业化还没有完全的落地，需要技术慢慢推进。

### 音视频编码技术

视频流采用的是 `H.264` 编码，主要是 `1080P@30fps`，`1080P@60fps` 。音频编码使用 `AAC`，因为标准的封装格式不含控制流，不能传输用户的操作数据，所以我们自己定义了一种封装格式，简单的把 `H.264` 和 `AAC` 的流封装起来传送给客户端。

目前用软件编码器基本不可行，一路视频编码就要消耗掉一个 CPU 核的资源，三四路就会占用掉全部 CPU 资源，软件就没办法运行了。幸运的是三大硬件厂商 Intel、AMD 和 NVIDIA 都推出了自己的硬件编码器，Intel 的 CPU 自带硬件编码器，支持 20+ 路的 720P 实时编码。NVIDIA 的硬件编码性能更高，可以直接对GPU的 `FrameBuffer` 编码并传到 CPU 上，节省了很多内存的拷贝，性能最高。

### 视频编码的参数调优

+   避免使用 B 帧以减小延迟
+   较大的 `GOP` 设置来减少帧的比例，保证每一帧消耗的码率都在一个最大可控的范围内
+   0 延迟设置，保证每输入一帧数据编码器都立刻输出此帧的编码数据，避免编码器缓冲帧数据
+   `bitrate` 控制，使用固定比特率的算法是不适合的，因为软件使用中经常会存在一段时间的静止画面，此时比特率很低，对接下来的变化帧编码器就会分配大量的比特来编码，这就会造成这一帧数据波动较大，从而带来了额外的网络数据传输延迟。所以我们采用了自适应算法，在保证比特率总体在最大范围内的同时，保证每一帧消耗的码率都在一个最大可控的范围内，确保每帧的数据传输延迟可控。

### 终端的视频解码优化

`H.264` 的解码比较困难，因为 `Android` 平台适配技术难度高，尤其是它的硬件解码规范度不高。如果直接使用 `mediacodec` 封装的硬件解码器，延迟会非常高，基本没有办法用。所以还是需要用软件解码的方式来支持 0 延迟的输出。`Android` 设备的性能参差不齐，早期的低端芯片性能不满足实时解码 ，需要利用 GPU 做一些加速。

### 网络传输的优化

+   UDP 传输，因为 H.264 本身不支持容错，一旦丢包就会出现花屏，在下一帧到来前都无法恢复，通常要持续好几秒，严重影响用户体验，无法接受

+   TCP 传输，丢包的话只是出现几百毫秒的卡顿，实测可以接受的、

以我们放弃了 UDP 协议传输，利用 TCP 在网络层做一些调优使延迟降低。实测中宽带网络延迟基本没有问题，主要问题在用户侧的 `WiFi` 上，一旦出现无线信号干扰，网络波动会比较厉害。

### 服务器和客户端的同步算法

我们完全关闭了软件运行时的缓冲阶段，均为零延迟直接输出，软件原本缓冲设计的目的就是为了抵抗网络波动和编解码环节抖动，通过缓冲将波动抹平。现在关闭缓冲后对同步会造成很大的影响。有很多因素会造成波动，比如服务器发送数据过快，客户端来不及接受，造成的结果就是延迟极高。所以我们设计了一套算法来解决同步的问题。具体的做法就是让客户端在完成一帧画面的显示后向服务器反馈消息，服务端根据客户端反馈的消息就知道客户端接受到了第几帧，再与服务器当前编码的帧数比较，在一定的阈值内则继续传输下一帧，否则等待客户端的确认消息，直到客户端来得及接受。这样做的结果就是当波动发生时服务器能及时感知并停止发送数据，等待波动消除后再继续发送最新的软件运行画面，实测中获得了比较理想的同步效果。

### 存储的优化

只读资源数据是放在 NAS 上的，上百个软件共享一个 NAS，加载软件时的网络 `I/O` 开销极大，所以我们针对共享文件的本地缓存做了专门优化，利用 `dokan` 实现了虚拟磁盘访问资源文件，再将虚拟磁盘重定向到 NAS 上，同时利用节点的本地 SSD 硬盘来缓存热点文件，从而降低了网络 I/O 的开销。

### 软件云服务挑战

云服务需要维护大量的服务器节点，而且与普通的服务器管理不一样，需要自己开发一些专有技术。由于所有的硬件资源都是高负荷运行，我们要最大化的增加硬件利用率，一般的服务器 CPU 占用不会超过 10% ，而云游戏的 CPU 是全程在接近 100% 的情况下运行，另外还需要 GPU 的参与，这导致了硬件可靠性的降低。

软件因为没有隔离性，可靠性也会降低，故障维护、恢复问题需要重视，因为市场上没有成熟方案，我们自主设计了服务器集群来解决这些问题，此外也解决了跨机房部署的问题。

硬件方案的选型，主要有三套方案：

+   GRID 显卡方案，这是 NVIDIA 为云游戏专门设计的专业显卡，带有编码器可以将软件画面直接编码输出，缺点是价格比较昂贵，一台服务器的硬件成本大约在 5 万元左右。

+   消费级独显方案，去掉了昂贵的专业显卡的同时还能获得更好的GPU性能，所以这套方案的性价比更高，每路并发的硬件成本可以降低到 500 元以下。

+   Intel 核显方案。完全不需要用独立显卡，但 Intel 核心显卡的性能偏弱，运行大型的 3D 软件会比较吃力，运行一些低压力软件没有问题。这个方案的优点是不需要显卡，1U 的尺寸下可以装入多个节点，集成度提高，而且易于维护，也是一个值得考虑的方案。

>   集群：Node（节点）对应一台物理计算机，一个节点可以同时运行多个游戏实例为用户提供服务。多个节点组成一个 Group（节点组），一个Group内包含了若干节点和NAS，对应于一个机柜， 多个机柜用万兆交换机串连起来，部署在一个机房，称之为 Cluster（集群），再上面一层是云游戏平台，包括用户的入口管理、登录计费等，可以跨越多个机房。

最终我们选择了消费级独显方案，并已在灰度测试用收到了较好的用户反馈。

